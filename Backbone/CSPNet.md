## CSPNet:Cross Stage Partial Network

CSPNet通过将梯度的变化从头到尾地集成到特征图中，在减少了计算量的同时可以保证准确率。CSPNet是一种处理的思想，可以和ResNet、ResNeXt和DenseNet结合。

The proposed networks respect the variability of the gradients by integrating feature maps from the beginning and the end of a network stage

The main purpose of designing CSPNet is to enable this architecture to **achieve a richer gradient combination while reducing the amount of computation**. **This aim is achieved by partitioning feature map of the base layer into two parts and then merging them through a proposed cross-stage hierarchy.** **Our main concept is to make the gradient flow propagate through different network paths by splitting the gradient flow.**

CSPNet作者也设计了几种特征融合的策略，如下图所示：

![img](https://img-blog.csdnimg.cn/2020032323093614.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0REX1BQX0pK,size_16,color_FFFFFF,t_70)

图中的Transition Layer代表过渡层，主要包含瓶颈层（1x1卷积）和池化层（可选）。（a）图是原始的DenseNet的特征融合方式。（b）图是CSPDenseNet的特征融合方式（trainsition->concatenation->transition）。（c）**图是Fusion First的特征融合方式（concatenation->transition）**（d）图是**Fusion Last的特征融合方式（transition->concatenation）**

Fustion First的方式是对两个分支的feature map先进行concatenation操作，这样梯度信息可以被重用。CSP (fusion first) means to concatenate the feature maps generated by two parts, and then do transition operation. If this strategy is adopted, a large amount of gradient information will be reused

Fusion Last的方式是对Dense Block所在分支先进性transition操作，然后再进行concatenation， 梯度信息将被截断，因此不会重复使用梯度信息 。As to the CSP (fusion last) strategy, the output from the dense block will go through the transition layer and then do concatenation with the feature map coming from part 1. If one goes with the CSP (fusion last) strategy, the gradient information will not be reused since the gradient flow is truncated

上图是对Fusion First、Fusion Last和CSP最终采用的融合方式(对应上图CSPPeleeNet)在ILSVRC2012分类数据集上的对比，可以得到以下结论：

- 使用Fusion First有助于降低计算代价，但是准确率有显著下降。
- 使用Fusion Last也是极大降低了计算代价，top-1 accuracy仅仅下降了0.1个百分点。
- 同时使用Fusion First和Fusion Last的CSP所采用的融合方式可以在降低计算代价的同时，提升准确率。

![img](https://img-blog.csdnimg.cn/20200323193956902.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0REX1BQX0pK,size_16,color_FFFFFF,t_70)

上图是DenseNet的示意图以及CSPDenseNet的改进，改进点在于CSPNet将浅层特征映射为两个部分，一部分经过Dense模块（图中的Partial Dense Block）,另一部分直接与Partial Dense Block输出进行concate。



Each stage of a DenseNet contains a dense block and a transition layer, and each dense block is composed of k dense layers. The output of the i th dense layer will be concatenated with the input of the i th dense layer, and the concatenated outcome will become the input of the (i + 1)th dense layer. The equations showing the above-mentioned mechanism can be expressed as



Cross Stage Partial DenseNet. The architecture of one-stage of the proposed CSPDenseNet is shown in Figure 2 (b). A stage of CSPDenseNet is composed of a partial dense block and a partial transition layer. In a partial dense block, the feature maps of the base layer in a stage are split into two parts through channel x0 = [x 0 0 , x 00 0 ]. Between x 00 0 and x 0 0 , the former is directly linked to the end of the stage, and the latter will go through a dense block. All steps involved in a partial transition layer are as follows: First, the output of dense layers, [x 00 0 , x1, ..., xk], will undergo a transition layer. Second, the output of this transition layer, xT , will be concatenated with x 00 0 and undergo another transition layer, and then generate output xU . The equations of feed-forward pass and weight updating of CSPDenseNet are shown in Equations 3 and 4, respectively



下图是将CSP模型应用到ResNeXt或者ResNet中：

![img](https://img-blog.csdnimg.cn/20200323222604673.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0REX1BQX0pK,size_16,color_FFFFFF,t_70)

跟CSPDenseNet一样，将上一层分为两部分，Part1不进行操作直接concate，Part2进行卷积操作。



**The purpose of designing partial dense blocks is to 1.) increase gradient path:** Through the split and merge strategy, the number of gradient paths can be doubled. Because of the cross-stage strategy, one can alleviate the disadvantages caused by using explicit feature map copy for concatenation; **2.) balance computation of each layer**: **usually, the channel number in the base layer of a DenseNet is much larger than the growth rate. Since the base layer channels involved in the dense layer operation in a partial dense block account for only half of the original number, it can effectively solve nearly half of the computational bottleneck;** and **3.) reduce memory traffic:** Assume the base feature map size of a dense block in a DenseNet is w × h × c, the growth rate is d, and there are in total m dense layers. Then, the CIO of that dense block is (c × m) + ((m2 + m) × d)/2, and the CIO of partial dense block is ((c × m) + (m2 + m) × d)/2. While m and d are usually far smaller than c, a partial dense block is able to save at most half of the memory traffic of a network.

##  FPN设计

![img](https://img-blog.csdnimg.cn/20200323222955690.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0REX1BQX0pK,size_16,color_FFFFFF,t_70)

论文中列举了三种FPN：

第一个如（a）图所示，是最常见的FPN，在YOLOv3中使用。（ps: YOLOv3中的FPN跟原始FPN不同，其融合的方式是concate）

第二个如（b）图所示，是ThunderNet中提出的GFM, 之前的文章中有详解，直接将多个不同分辨率的特征进行融合，具体融合方式是相加。

第三个如（c）图所示，是EFM，也就是本文提出的融合方式，没两层之间特征进行融合，并且在原有FPN基础上再进行一次bottom-up的融合方式。