**ResNets要解决的是深度神经网络的“退化”问题。**

什么是“退化”？

我们知道，对浅层网络逐渐叠加layers，模型在训练集和测试集上的性能会变好，因为模型复杂度更高了，表达能力更强了，可以对潜在的映射关系拟合得更好。而**“退化”指的是，给网络叠加更多的层后，性能却快速下降的情况。**

**训练集上的性能下降，可以排除过拟合，BN层的引入也基本解决了plain net的梯度消失和梯度爆炸问题。**如果不是过拟合以及梯度消失导致的，那原因是什么？

按道理，给网络叠加更多层，浅层网络的解空间是包含在深层网络的解空间中的，深层网络的解空间至少存在不差于浅层网络的解，因为只需将增加的层变成恒等映射，其他层的权重原封不动copy浅层网络，就可以获得与浅层网络同样的性能。**更好的解明明存在，为什么找不到？找到的反而是更差的解？**

显然，这是个优化问题，反映出结构相似的模型，其优化难度是不一样的，且难度的增长并不是线性的，越深的模型越难以优化。

有两种解决思路，**一种是调整求解方法，比如更好的初始化、更好的梯度下降算法等；另一种是调整模型结构，让模型更易于优化——改变模型结构实际上是改变了error surface的形态。**

ResNet的作者从后者入手，探求更好的模型结构。将堆叠的几层layer称之为一个block，对于某个block，其可以拟合的函数为𝐹(𝑥)，如果期望的潜在映射为𝐻(𝑥)，**与其让𝐹(𝑥)直接学习潜在的映射，不如去学习残差𝐻(𝑥)−𝑥，即𝐹(𝑥):=𝐻(𝑥)−𝑥，这样原本的前向路径上就变成了𝐹(𝑥)+𝑥，用𝐹(𝑥)+𝑥来拟合𝐻(𝑥)。**作者认为这样可能更易于优化，因为**相比于让𝐹(𝑥)学习成恒等映射，让𝐹(𝑥)学习成0要更加容易——后者通过L2正则就可以轻松实现。**这样，对于冗余的block，只需𝐹(𝑥)→0就可以得到恒等映射，性能不减。



# Residual Block的设计

𝐹(𝑥)+𝑥构成的block称之为**Residual Block**，即**残差块**，如下图所示，**多个相似的Residual Block串联构成ResNet**。

**注意： 这些weight layer 即残差部分，也是一个一个的块，由几层conv或加上其他的组成，详细见下图蓝色的部分。**

[![3uUio4.png](https://s2.ax1x.com/2020/02/21/3uUio4.png)](https://s2.ax1x.com/2020/02/21/3uUio4.png)

一个残差块有2条路径𝐹(𝑥)和𝑥，𝐹(𝑥)路径拟合残差，不妨称之为残差路径，𝑥路径为identity mapping恒等映射，称之为”shortcut”。**图中的⊕⊕为element-wise addition，要求参与运算的𝐹(𝑥)和𝑥的尺寸要相同**。所以，随之而来的问题是，

- 残差路径如何设计？
- shortcut路径如何设计？
- Residual Block之间怎么连接？

在原论文中，残差路径可以大致分成2种，一种有**bottleneck**结构，即下图右中的1×1 卷积层，用于先降维再升维，主要出于**降低计算复杂度的现实考虑**，称之为“**bottleneck block**”，另一种没有bottleneck结构，如下图左所示，称之为“**basic block**”。basic block由2个3×3卷积层构成，bottleneck block由1×1

[![3K34c8.png](https://s2.ax1x.com/2020/02/21/3K34c8.png)](https://s2.ax1x.com/2020/02/21/3K34c8.png)

shortcut路径大致也可以分成2种，取决于残差路径是否改变了feature map数量和尺寸，一种是将输入𝑥x原封不动地输出，**另一种则需要经过1×1卷积来升维 or/and 降采样**，主要作用是**将输出与𝐹(𝑥)路径的输出保持shape一致**，对网络性能的提升并不明显，两种结构如下图所示，

[![https://d2l.ai/chapter_convolutional-modern/resnet.html](https://s2.ax1x.com/2020/02/23/3l4cD0.png)](https://s2.ax1x.com/2020/02/23/3l4cD0.png)

至于Residual Block之间的衔接，在原论文中，𝐹(𝑥)+𝑥经过𝑅𝑒𝐿𝑈后直接作为下一个block的输入𝑥。

